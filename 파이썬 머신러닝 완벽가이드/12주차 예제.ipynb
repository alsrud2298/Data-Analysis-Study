{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris 데이터로 랜덤 포레스트 구현해보기\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Training data와 Test 데이터 나누기 (train:test=8:2, random_state=123)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "iris_df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "iris_df['target'] = y\n",
    "X_features = iris_df.iloc[:,:-1]\n",
    "y_labels = iris_df.iloc[:,-1]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 랜덤 포레스트 분류기 생성, 학습, 예측\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# n_estimators=10\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(X_train,y_train)\n",
    "pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.75      0.86         8\n",
      "           2       0.82      1.00      0.90         9\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.92      0.92        30\n",
      "weighted avg       0.95      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. 랜덤 포레스트 분류기의 성능 평가\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.17061923947376811\n",
      "sepal width (cm) 0.027622325107367895\n",
      "petal length (cm) 0.3276518735556894\n",
      "petal width (cm) 0.47410656186317457\n"
     ]
    }
   ],
   "source": [
    "# 피처 중요도 확인하기\n",
    "for feature, imp in zip(iris.feature_names, rfc.feature_importances_):\n",
    "    print(feature, imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 구남이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>diameter</th>\n",
       "      <th>weight</th>\n",
       "      <th>red</th>\n",
       "      <th>green</th>\n",
       "      <th>blue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orange</td>\n",
       "      <td>2.96</td>\n",
       "      <td>86.76</td>\n",
       "      <td>172</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orange</td>\n",
       "      <td>3.91</td>\n",
       "      <td>88.05</td>\n",
       "      <td>166</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>orange</td>\n",
       "      <td>4.42</td>\n",
       "      <td>95.17</td>\n",
       "      <td>156</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>orange</td>\n",
       "      <td>4.47</td>\n",
       "      <td>95.60</td>\n",
       "      <td>163</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>orange</td>\n",
       "      <td>4.48</td>\n",
       "      <td>95.76</td>\n",
       "      <td>161</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>grapefruit</td>\n",
       "      <td>15.35</td>\n",
       "      <td>253.89</td>\n",
       "      <td>149</td>\n",
       "      <td>77</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>grapefruit</td>\n",
       "      <td>15.41</td>\n",
       "      <td>254.67</td>\n",
       "      <td>148</td>\n",
       "      <td>68</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>grapefruit</td>\n",
       "      <td>15.59</td>\n",
       "      <td>256.50</td>\n",
       "      <td>168</td>\n",
       "      <td>82</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>grapefruit</td>\n",
       "      <td>15.92</td>\n",
       "      <td>260.14</td>\n",
       "      <td>142</td>\n",
       "      <td>72</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>grapefruit</td>\n",
       "      <td>16.45</td>\n",
       "      <td>261.51</td>\n",
       "      <td>152</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  diameter  weight  red  green  blue\n",
       "0         orange      2.96   86.76  172     85     2\n",
       "1         orange      3.91   88.05  166     78     3\n",
       "2         orange      4.42   95.17  156     81     2\n",
       "3         orange      4.47   95.60  163     81     4\n",
       "4         orange      4.48   95.76  161     72     9\n",
       "...          ...       ...     ...  ...    ...   ...\n",
       "9995  grapefruit     15.35  253.89  149     77    20\n",
       "9996  grapefruit     15.41  254.67  148     68     7\n",
       "9997  grapefruit     15.59  256.50  168     82    20\n",
       "9998  grapefruit     15.92  260.14  142     72    11\n",
       "9999  grapefruit     16.45  261.51  152     74     2\n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "citrus_df = pd.read_csv('./data/citrus.csv')\n",
    "citrus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. 'name열의 'orange'는 1, 'grapefruit'은 0값을 가지도록 바꿔주고 y_citrus_df에 저장하기\n",
    "y_citrus_df = citrus_df.replace({'orange':1,'grapefruit':0})['name']\n",
    "\n",
    "# Q2. 피처 값으로 이루어진 X_citrus_df 만들기\n",
    "X_citrus_df = citrus_df.drop('name',axis=1)\n",
    "\n",
    "# Q3. 훈련 데이터 / 테스트 데이터로 구분\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_citrus_df, y_citrus_df, test_size=0.2, random_state=0)\n",
    "\n",
    "lr_clf = LogisticRegression(random_state=0)\n",
    "lr_clf.fit(X_train , y_train)\n",
    "pred = lr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. pred_proba에 1에 대한 예측확률 저장하기\n",
    "pred_proba = lr_clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차 행렬\n",
      "[[977  35]\n",
      " [ 58 930]]\n",
      "정확도: 0.9535, 정밀도: 0.9637, 재현율: 0.9413,    F1: 0.9524, AUC:0.9870\n"
     ]
    }
   ],
   "source": [
    "# Q5. 오차행렬, 정확도, 정밀도, 재현율, F1 스코어 출력하기\n",
    "get_clf_eval(y_test , pred, pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 강수민"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "diabetes_data = pd.read_csv('./data/diabetes.csv') # 교재 실습 데이터\n",
    "diabetes_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "zero_features = ['Glucose', 'BloodPressure','SkinThickness','Insulin','BMI']\n",
    "# zero_features 리스트 내부에 저장된 개별 피처들에 대해서 0값을 평균 값으로 대체\n",
    "mean_zero_features = diabetes_data[zero_features].mean()\n",
    "diabetes_data[zero_features]=diabetes_data[zero_features].replace(0, mean_zero_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. 당뇨병 데이터 셋을 마지막 열(Outcome)을 y로, 이를 제외한 모든 열을 X로 설정하라.\n",
    "X = diabetes_data.iloc[:,:-1]\n",
    "y = diabetes_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. train_test_split를 이용해 학습용, 테스트용 데이터를 나눠라. (원래 데이터 분포와 유사하게 추출하기, test_size=0.2)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'presort': 'deprecated',\n",
       " 'random_state': 0,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3. 하이퍼 파라미터를 디폴트로 설정해 결정트리 분류를 시행하고 이 때의 기본 하이퍼 파라미터를 추출해라.\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "dt_clf.fit(X_train,y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "accuracy_score(y_test, pred)\n",
    "params = dt_clf.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터:\n",
      " {'max_depth': 8, 'min_samples_split': 16}\n",
      "최고 예측 정확도: 0.713288018126083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    8.7s finished\n"
     ]
    }
   ],
   "source": [
    "# Q4. 하이퍼 파라미터 튜닝을 통해 최고 평균 정확도 수치와 최적 하이퍼 파라미터 추출하기\n",
    "\n",
    "params = {\n",
    "    'max_depth': [8, 12, 16, 20], \n",
    "    'min_samples_split': [16, 24],\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print('최적 하이퍼 파라미터:\\n',grid_cv.best_params_)\n",
    "print('최고 예측 정확도:',grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532467532467533"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5. grid_cv의 최적 추정량을 찾아 다시 결정 트리의 예측 정확도 값을 추출해라\n",
    "\n",
    "best_df_clf = grid_cv.best_estimator_\n",
    "pred1 = best_df_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred1)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q6. 피처별 중요도 그래프로 나타내기(5개만 뽑아 나타내기)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ftr_importances_values = best_df_clf.feature_importances_\n",
    "# Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환\n",
    "ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns  )\n",
    "# 중요도값 순으로 Series를 정렬\n",
    "ftr_top5 = ftr_importances.sort_values(ascending=False)[:5]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature importances Top 5')\n",
    "sns.barplot(x=ftr_top5 , y = ftr_top5.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 박민영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iris 데이터에 XGBoost 알고리즘(파이썬 래퍼를 이용)을 적용하고자 한다.\n",
    "\n",
    "# 1. 데이터를 불러오기 위해 아래 빈칸을 채워라.\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_iris()\n",
    "features = dataset.data\n",
    "labels = dataset.target\n",
    "\n",
    "iris_df = pd.DataFrame(data = features, columns = dataset.feature_names)\n",
    "iris_df['target'] = labels\n",
    "iris_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터를 학습, 검증, 예측 데이터셋으로 분할하기 위해 아래 빈칸을 채워라.\n",
    "# 단, 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터로 분리\n",
    "# 80%의 학습용 데이터를 다시 분리하여 90%는 학습용 데이터, 10%는 검증용 데이터로 분리\n",
    "\n",
    "X_features = iris_df.iloc[:,:-1]\n",
    "y_label = iris_df.iloc[:,-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size = 0.2, random_state = 156)\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. [파이썬 wrapper] 학습, 검증, 예측 데이터 세트를 DMatrix 객체인 dtr, dval, dtest로 변환하여라.\n",
    "\n",
    "import xgboost as xgb\n",
    "dtr = xgb.DMatrix(data=X_tr, label=y_tr)\n",
    "dval = xgb.DMatrix(data=X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.52515\teval-logloss:0.57417\n",
      "[1]\ttrain-logloss:0.36788\teval-logloss:0.46432\n",
      "[2]\ttrain-logloss:0.21566\teval-logloss:0.35978\n",
      "[3]\ttrain-logloss:0.06264\teval-logloss:0.25678\n",
      "[4]\ttrain-logloss:-0.09918\teval-logloss:0.15104\n",
      "[5]\ttrain-logloss:-0.28442\teval-logloss:0.03327\n",
      "[6]\ttrain-logloss:-0.53285\teval-logloss:-0.11976\n",
      "[7]\ttrain-logloss:-1.15778\teval-logloss:-0.49001\n",
      "[8]\ttrain-logloss:-10.38971\teval-logloss:-5.85320\n",
      "[9]\ttrain-logloss:-10.42178\teval-logloss:-5.87000\n",
      "[10]\ttrain-logloss:-10.45012\teval-logloss:-5.88564\n",
      "[11]\ttrain-logloss:-10.48881\teval-logloss:-5.90023\n",
      "[12]\ttrain-logloss:-10.52963\teval-logloss:-5.91385\n",
      "[13]\ttrain-logloss:-11.46938\teval-logloss:-5.92658\n",
      "[14]\ttrain-logloss:-11.49434\teval-logloss:-5.93849\n",
      "[15]\ttrain-logloss:-11.81147\teval-logloss:-5.94946\n",
      "[16]\ttrain-logloss:-12.12911\teval-logloss:-5.95975\n",
      "[17]\ttrain-logloss:-12.13713\teval-logloss:-5.96941\n",
      "[18]\ttrain-logloss:-12.14505\teval-logloss:-5.97879\n",
      "[19]\ttrain-logloss:-12.15212\teval-logloss:-5.98731\n",
      "[20]\ttrain-logloss:-12.15877\teval-logloss:-5.99532\n",
      "[21]\ttrain-logloss:-12.16538\teval-logloss:-6.00315\n",
      "[22]\ttrain-logloss:-12.17183\teval-logloss:-6.01071\n",
      "[23]\ttrain-logloss:-12.17768\teval-logloss:-6.01764\n",
      "[24]\ttrain-logloss:-12.18340\teval-logloss:-6.02435\n",
      "[25]\ttrain-logloss:-12.18827\teval-logloss:-6.03024\n",
      "[26]\ttrain-logloss:-12.19318\teval-logloss:-6.03606\n",
      "[27]\ttrain-logloss:-12.19797\teval-logloss:-6.04169\n",
      "[28]\ttrain-logloss:-12.20203\teval-logloss:-6.04661\n",
      "[29]\ttrain-logloss:-12.20677\teval-logloss:-6.05217\n",
      "[30]\ttrain-logloss:-12.21035\teval-logloss:-6.05654\n",
      "[31]\ttrain-logloss:-12.21414\teval-logloss:-6.06100\n",
      "[32]\ttrain-logloss:-12.21732\teval-logloss:-6.06489\n",
      "[33]\ttrain-logloss:-12.22071\teval-logloss:-6.06888\n",
      "[34]\ttrain-logloss:-12.22355\teval-logloss:-6.07235\n",
      "[35]\ttrain-logloss:-12.22658\teval-logloss:-6.07592\n",
      "[36]\ttrain-logloss:-12.22911\teval-logloss:-6.07902\n",
      "[37]\ttrain-logloss:-12.23183\teval-logloss:-6.08223\n",
      "[38]\ttrain-logloss:-12.23408\teval-logloss:-6.08500\n",
      "[39]\ttrain-logloss:-12.23652\teval-logloss:-6.08787\n",
      "[40]\ttrain-logloss:-12.23852\teval-logloss:-6.09035\n",
      "[41]\ttrain-logloss:-12.24075\teval-logloss:-6.09295\n",
      "[42]\ttrain-logloss:-12.24285\teval-logloss:-6.09542\n",
      "[43]\ttrain-logloss:-12.24480\teval-logloss:-6.09773\n",
      "[44]\ttrain-logloss:-12.24669\teval-logloss:-6.09995\n",
      "[45]\ttrain-logloss:-12.24818\teval-logloss:-6.10180\n",
      "[46]\ttrain-logloss:-12.24987\teval-logloss:-6.10379\n",
      "[47]\ttrain-logloss:-12.25177\teval-logloss:-6.10606\n",
      "[48]\ttrain-logloss:-12.25301\teval-logloss:-6.10762\n",
      "[49]\ttrain-logloss:-12.25443\teval-logloss:-6.10930\n",
      "[50]\ttrain-logloss:-12.25606\teval-logloss:-6.11126\n",
      "[51]\ttrain-logloss:-12.25708\teval-logloss:-6.11256\n",
      "[52]\ttrain-logloss:-12.25856\teval-logloss:-6.11433\n",
      "[53]\ttrain-logloss:-12.25946\teval-logloss:-6.11541\n",
      "[54]\ttrain-logloss:-12.26032\teval-logloss:-6.11643\n",
      "[55]\ttrain-logloss:-12.26113\teval-logloss:-6.11741\n",
      "[56]\ttrain-logloss:-12.26190\teval-logloss:-6.11833\n",
      "[57]\ttrain-logloss:-12.26264\teval-logloss:-6.11921\n",
      "[58]\ttrain-logloss:-12.26333\teval-logloss:-6.12004\n",
      "[59]\ttrain-logloss:-12.26400\teval-logloss:-6.12084\n",
      "[60]\ttrain-logloss:-12.26463\teval-logloss:-6.12159\n",
      "[61]\ttrain-logloss:-12.26523\teval-logloss:-6.12231\n",
      "[62]\ttrain-logloss:-12.26580\teval-logloss:-6.12299\n",
      "[63]\ttrain-logloss:-12.26634\teval-logloss:-6.12364\n",
      "[64]\ttrain-logloss:-12.26686\teval-logloss:-6.12425\n",
      "[65]\ttrain-logloss:-12.26735\teval-logloss:-6.12484\n",
      "[66]\ttrain-logloss:-12.26781\teval-logloss:-6.12540\n",
      "[67]\ttrain-logloss:-12.26826\teval-logloss:-6.12592\n",
      "[68]\ttrain-logloss:-12.26868\teval-logloss:-6.12643\n",
      "[69]\ttrain-logloss:-12.26928\teval-logloss:-6.12725\n",
      "[70]\ttrain-logloss:-12.26966\teval-logloss:-6.12770\n",
      "[71]\ttrain-logloss:-12.27018\teval-logloss:-6.12844\n",
      "[72]\ttrain-logloss:-12.27052\teval-logloss:-6.12883\n",
      "[73]\ttrain-logloss:-12.27101\teval-logloss:-6.12951\n",
      "[74]\ttrain-logloss:-12.27130\teval-logloss:-6.12985\n",
      "[75]\ttrain-logloss:-12.27174\teval-logloss:-6.13047\n",
      "[76]\ttrain-logloss:-12.27177\teval-logloss:-6.13045\n",
      "[77]\ttrain-logloss:-12.27218\teval-logloss:-6.13103\n",
      "[78]\ttrain-logloss:-12.27221\teval-logloss:-6.13101\n",
      "[79]\ttrain-logloss:-12.27224\teval-logloss:-6.13099\n",
      "[80]\ttrain-logloss:-12.27226\teval-logloss:-6.13097\n",
      "[81]\ttrain-logloss:-12.27266\teval-logloss:-6.13143\n",
      "[82]\ttrain-logloss:-12.27304\teval-logloss:-6.13186\n",
      "[83]\ttrain-logloss:-12.27307\teval-logloss:-6.13182\n",
      "[84]\ttrain-logloss:-12.27342\teval-logloss:-6.13223\n",
      "[85]\ttrain-logloss:-12.27344\teval-logloss:-6.13219\n",
      "[86]\ttrain-logloss:-12.27377\teval-logloss:-6.13257\n",
      "[87]\ttrain-logloss:-12.27412\teval-logloss:-6.13288\n",
      "[88]\ttrain-logloss:-12.27414\teval-logloss:-6.13288\n",
      "[89]\ttrain-logloss:-12.27403\teval-logloss:-6.13282\n",
      "[90]\ttrain-logloss:-12.27432\teval-logloss:-6.13316\n",
      "[91]\ttrain-logloss:-12.27463\teval-logloss:-6.13343\n",
      "[92]\ttrain-logloss:-12.27453\teval-logloss:-6.13337\n",
      "[93]\ttrain-logloss:-12.27454\teval-logloss:-6.13336\n",
      "[94]\ttrain-logloss:-12.27457\teval-logloss:-6.13345\n",
      "[95]\ttrain-logloss:-12.27447\teval-logloss:-6.13340\n",
      "[96]\ttrain-logloss:-12.27449\teval-logloss:-6.13349\n",
      "[97]\ttrain-logloss:-12.27452\teval-logloss:-6.13357\n",
      "[98]\ttrain-logloss:-12.27454\teval-logloss:-6.13365\n",
      "[99]\ttrain-logloss:-12.27456\teval-logloss:-6.13372\n",
      "[100]\ttrain-logloss:-12.27458\teval-logloss:-6.13380\n",
      "[101]\ttrain-logloss:-12.27461\teval-logloss:-6.13386\n",
      "[102]\ttrain-logloss:-12.27462\teval-logloss:-6.13393\n",
      "[103]\ttrain-logloss:-12.27464\teval-logloss:-6.13399\n",
      "[104]\ttrain-logloss:-12.27466\teval-logloss:-6.13405\n",
      "[105]\ttrain-logloss:-12.27463\teval-logloss:-6.13399\n",
      "[106]\ttrain-logloss:-12.27481\teval-logloss:-6.13423\n",
      "[107]\ttrain-logloss:-12.27482\teval-logloss:-6.13429\n",
      "[108]\ttrain-logloss:-12.27484\teval-logloss:-6.13435\n",
      "[109]\ttrain-logloss:-12.27500\teval-logloss:-6.13457\n",
      "[110]\ttrain-logloss:-12.27499\teval-logloss:-6.13450\n",
      "[111]\ttrain-logloss:-12.27501\teval-logloss:-6.13456\n",
      "[112]\ttrain-logloss:-12.27516\teval-logloss:-6.13477\n",
      "[113]\ttrain-logloss:-12.27515\teval-logloss:-6.13470\n",
      "[114]\ttrain-logloss:-12.27512\teval-logloss:-6.13466\n",
      "[115]\ttrain-logloss:-12.27508\teval-logloss:-6.13463\n",
      "[116]\ttrain-logloss:-12.27537\teval-logloss:-6.13496\n",
      "[117]\ttrain-logloss:-12.27538\teval-logloss:-6.13503\n",
      "[118]\ttrain-logloss:-12.27540\teval-logloss:-6.13509\n",
      "[119]\ttrain-logloss:-12.27539\teval-logloss:-6.13502\n",
      "[120]\ttrain-logloss:-12.27566\teval-logloss:-6.13533\n",
      "[121]\ttrain-logloss:-12.27563\teval-logloss:-6.13532\n",
      "[122]\ttrain-logloss:-12.27562\teval-logloss:-6.13526\n",
      "[123]\ttrain-logloss:-12.27588\teval-logloss:-6.13556\n",
      "[124]\ttrain-logloss:-12.27595\teval-logloss:-6.13560\n",
      "[125]\ttrain-logloss:-12.27586\teval-logloss:-6.13556\n",
      "[126]\ttrain-logloss:-12.27577\teval-logloss:-6.13552\n",
      "[127]\ttrain-logloss:-12.27569\teval-logloss:-6.13548\n",
      "[128]\ttrain-logloss:-12.27569\teval-logloss:-6.13543\n",
      "[129]\ttrain-logloss:-12.27581\teval-logloss:-6.13569\n",
      "[130]\ttrain-logloss:-12.27580\teval-logloss:-6.13564\n",
      "[131]\ttrain-logloss:-12.27604\teval-logloss:-6.13593\n",
      "[132]\ttrain-logloss:-12.27596\teval-logloss:-6.13589\n",
      "[133]\ttrain-logloss:-12.27617\teval-logloss:-6.13623\n",
      "[134]\ttrain-logloss:-12.27618\teval-logloss:-6.13625\n",
      "[135]\ttrain-logloss:-12.27617\teval-logloss:-6.13620\n",
      "[136]\ttrain-logloss:-12.27616\teval-logloss:-6.13616\n",
      "[137]\ttrain-logloss:-12.27615\teval-logloss:-6.13612\n",
      "[138]\ttrain-logloss:-12.27636\teval-logloss:-6.13646\n",
      "[139]\ttrain-logloss:-12.27628\teval-logloss:-6.13642\n",
      "[140]\ttrain-logloss:-12.27647\teval-logloss:-6.13674\n",
      "[141]\ttrain-logloss:-12.27639\teval-logloss:-6.13671\n",
      "[142]\ttrain-logloss:-12.27637\teval-logloss:-6.13666\n",
      "[143]\ttrain-logloss:-12.27656\teval-logloss:-6.13698\n",
      "[144]\ttrain-logloss:-12.27649\teval-logloss:-6.13695\n",
      "[145]\ttrain-logloss:-12.27647\teval-logloss:-6.13689\n",
      "[146]\ttrain-logloss:-12.27659\teval-logloss:-6.13711\n",
      "[147]\ttrain-logloss:-12.27658\teval-logloss:-6.13706\n",
      "[148]\ttrain-logloss:-12.27669\teval-logloss:-6.13728\n",
      "[149]\ttrain-logloss:-12.27666\teval-logloss:-6.13724\n",
      "[150]\ttrain-logloss:-12.27676\teval-logloss:-6.13744\n",
      "[151]\ttrain-logloss:-12.27676\teval-logloss:-6.13744\n",
      "[152]\ttrain-logloss:-12.27673\teval-logloss:-6.13739\n",
      "[153]\ttrain-logloss:-12.27683\teval-logloss:-6.13760\n",
      "[154]\ttrain-logloss:-12.27680\teval-logloss:-6.13756\n",
      "[155]\ttrain-logloss:-12.27693\teval-logloss:-6.13781\n",
      "[156]\ttrain-logloss:-12.27685\teval-logloss:-6.13777\n",
      "[157]\ttrain-logloss:-12.27694\teval-logloss:-6.13789\n",
      "[158]\ttrain-logloss:-12.27694\teval-logloss:-6.13788\n",
      "[159]\ttrain-logloss:-12.27692\teval-logloss:-6.13783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160]\ttrain-logloss:-12.27689\teval-logloss:-6.13777\n",
      "[161]\ttrain-logloss:-12.27695\teval-logloss:-6.13789\n",
      "[162]\ttrain-logloss:-12.27692\teval-logloss:-6.13782\n",
      "[163]\ttrain-logloss:-12.27701\teval-logloss:-6.13794\n",
      "[164]\ttrain-logloss:-12.27701\teval-logloss:-6.13794\n",
      "[165]\ttrain-logloss:-12.27706\teval-logloss:-6.13805\n",
      "[166]\ttrain-logloss:-12.27703\teval-logloss:-6.13799\n",
      "[167]\ttrain-logloss:-12.27711\teval-logloss:-6.13810\n",
      "[168]\ttrain-logloss:-12.27708\teval-logloss:-6.13804\n",
      "[169]\ttrain-logloss:-12.27716\teval-logloss:-6.13815\n",
      "[170]\ttrain-logloss:-12.27716\teval-logloss:-6.13814\n",
      "[171]\ttrain-logloss:-12.27722\teval-logloss:-6.13825\n",
      "[172]\ttrain-logloss:-12.27718\teval-logloss:-6.13818\n",
      "[173]\ttrain-logloss:-12.27711\teval-logloss:-6.13817\n",
      "[174]\ttrain-logloss:-12.27709\teval-logloss:-6.13811\n",
      "[175]\ttrain-logloss:-12.27709\teval-logloss:-6.13810\n",
      "[176]\ttrain-logloss:-12.27716\teval-logloss:-6.13814\n",
      "[177]\ttrain-logloss:-12.27723\teval-logloss:-6.13824\n",
      "[178]\ttrain-logloss:-12.27721\teval-logloss:-6.13819\n",
      "[179]\ttrain-logloss:-12.27727\teval-logloss:-6.13829\n",
      "[180]\ttrain-logloss:-12.27725\teval-logloss:-6.13823\n",
      "[181]\ttrain-logloss:-12.27734\teval-logloss:-6.13833\n",
      "[182]\ttrain-logloss:-12.27740\teval-logloss:-6.13842\n",
      "[183]\ttrain-logloss:-12.27740\teval-logloss:-6.13842\n",
      "[184]\ttrain-logloss:-12.27737\teval-logloss:-6.13837\n",
      "[185]\ttrain-logloss:-12.27730\teval-logloss:-6.13834\n",
      "[186]\ttrain-logloss:-12.27736\teval-logloss:-6.13843\n",
      "[187]\ttrain-logloss:-12.27736\teval-logloss:-6.13843\n",
      "[188]\ttrain-logloss:-12.27734\teval-logloss:-6.13838\n",
      "[189]\ttrain-logloss:-12.27729\teval-logloss:-6.13842\n",
      "[190]\ttrain-logloss:-12.27737\teval-logloss:-6.13845\n",
      "[191]\ttrain-logloss:-12.27742\teval-logloss:-6.13854\n",
      "[192]\ttrain-logloss:-12.27741\teval-logloss:-6.13849\n",
      "[193]\ttrain-logloss:-12.27746\teval-logloss:-6.13857\n",
      "[194]\ttrain-logloss:-12.27746\teval-logloss:-6.13857\n",
      "[195]\ttrain-logloss:-12.27744\teval-logloss:-6.13852\n",
      "[196]\ttrain-logloss:-12.27749\teval-logloss:-6.13861\n",
      "[197]\ttrain-logloss:-12.27749\teval-logloss:-6.13860\n",
      "[198]\ttrain-logloss:-12.27743\teval-logloss:-6.13857\n",
      "[199]\ttrain-logloss:-12.27748\teval-logloss:-6.13865\n",
      "[200]\ttrain-logloss:-12.27739\teval-logloss:-6.13869\n",
      "[201]\ttrain-logloss:-12.27738\teval-logloss:-6.13863\n",
      "[202]\ttrain-logloss:-12.27737\teval-logloss:-6.13859\n",
      "[203]\ttrain-logloss:-12.27742\teval-logloss:-6.13866\n",
      "[204]\ttrain-logloss:-12.27736\teval-logloss:-6.13863\n",
      "[205]\ttrain-logloss:-12.27744\teval-logloss:-6.13866\n",
      "[206]\ttrain-logloss:-12.27754\teval-logloss:-6.13872\n",
      "[207]\ttrain-logloss:-12.27753\teval-logloss:-6.13870\n",
      "[208]\ttrain-logloss:-12.27758\teval-logloss:-6.13877\n",
      "[209]\ttrain-logloss:-12.27752\teval-logloss:-6.13859\n",
      "[210]\ttrain-logloss:-12.27760\teval-logloss:-6.13867\n",
      "[211]\ttrain-logloss:-12.27766\teval-logloss:-6.13870\n",
      "[212]\ttrain-logloss:-12.27772\teval-logloss:-6.13872\n",
      "[213]\ttrain-logloss:-12.27779\teval-logloss:-6.13879\n",
      "[214]\ttrain-logloss:-12.27784\teval-logloss:-6.13880\n",
      "[215]\ttrain-logloss:-12.27783\teval-logloss:-6.13856\n",
      "[216]\ttrain-logloss:-12.27782\teval-logloss:-6.13829\n",
      "[217]\ttrain-logloss:-12.27781\teval-logloss:-6.13803\n",
      "[218]\ttrain-logloss:-12.27787\teval-logloss:-6.13811\n",
      "[219]\ttrain-logloss:-12.27793\teval-logloss:-6.13814\n",
      "[220]\ttrain-logloss:-12.27798\teval-logloss:-6.13817\n",
      "[221]\ttrain-logloss:-12.27801\teval-logloss:-6.13816\n",
      "[222]\ttrain-logloss:-12.27807\teval-logloss:-6.13824\n",
      "[223]\ttrain-logloss:-12.27812\teval-logloss:-6.13827\n",
      "[224]\ttrain-logloss:-12.27818\teval-logloss:-6.13830\n",
      "[225]\ttrain-logloss:-12.27814\teval-logloss:-6.13826\n",
      "[226]\ttrain-logloss:-12.27813\teval-logloss:-6.13800\n",
      "[227]\ttrain-logloss:-12.27805\teval-logloss:-6.13803\n",
      "[228]\ttrain-logloss:-12.27811\teval-logloss:-6.13810\n",
      "[229]\ttrain-logloss:-12.27817\teval-logloss:-6.13813\n",
      "[230]\ttrain-logloss:-12.27813\teval-logloss:-6.13810\n",
      "[231]\ttrain-logloss:-12.27810\teval-logloss:-6.13807\n",
      "[232]\ttrain-logloss:-12.27802\teval-logloss:-6.13809\n",
      "[233]\ttrain-logloss:-12.27806\teval-logloss:-6.13811\n",
      "[234]\ttrain-logloss:-12.27810\teval-logloss:-6.13813\n",
      "[235]\ttrain-logloss:-12.27814\teval-logloss:-6.13814\n",
      "[236]\ttrain-logloss:-12.27818\teval-logloss:-6.13819\n",
      "[237]\ttrain-logloss:-12.27816\teval-logloss:-6.13823\n",
      "[238]\ttrain-logloss:-12.27820\teval-logloss:-6.13826\n",
      "[239]\ttrain-logloss:-12.27817\teval-logloss:-6.13831\n",
      "[240]\ttrain-logloss:-12.27820\teval-logloss:-6.13832\n",
      "[241]\ttrain-logloss:-12.27824\teval-logloss:-6.13832\n",
      "[242]\ttrain-logloss:-12.27828\teval-logloss:-6.13836\n",
      "[243]\ttrain-logloss:-12.27825\teval-logloss:-6.13839\n",
      "[244]\ttrain-logloss:-12.27828\teval-logloss:-6.13839\n",
      "[245]\ttrain-logloss:-12.27832\teval-logloss:-6.13843\n",
      "[246]\ttrain-logloss:-12.27833\teval-logloss:-6.13843\n",
      "[247]\ttrain-logloss:-12.27833\teval-logloss:-6.13844\n",
      "[248]\ttrain-logloss:-12.27836\teval-logloss:-6.13844\n",
      "[249]\ttrain-logloss:-12.27838\teval-logloss:-6.13844\n",
      "[250]\ttrain-logloss:-12.27839\teval-logloss:-6.13844\n",
      "[251]\ttrain-logloss:-12.27840\teval-logloss:-6.13844\n",
      "[252]\ttrain-logloss:-12.27841\teval-logloss:-6.13845\n",
      "[253]\ttrain-logloss:-12.27843\teval-logloss:-6.13845\n",
      "[254]\ttrain-logloss:-12.27844\teval-logloss:-6.13846\n",
      "[255]\ttrain-logloss:-12.27845\teval-logloss:-6.13845\n",
      "[256]\ttrain-logloss:-12.27846\teval-logloss:-6.13846\n",
      "[257]\ttrain-logloss:-12.27848\teval-logloss:-6.13846\n",
      "[258]\ttrain-logloss:-12.27848\teval-logloss:-6.13846\n",
      "[259]\ttrain-logloss:-12.27850\teval-logloss:-6.13846\n",
      "[260]\ttrain-logloss:-12.27852\teval-logloss:-6.13849\n",
      "[261]\ttrain-logloss:-12.27853\teval-logloss:-6.13849\n",
      "[262]\ttrain-logloss:-12.27855\teval-logloss:-6.13851\n",
      "[263]\ttrain-logloss:-12.27854\teval-logloss:-6.13840\n"
     ]
    }
   ],
   "source": [
    "# 4. [파이썬 wrapper]설정한 하이퍼 파라미터와 early stopping 파라미터를 전달하여 학습하기 위해 아래 빈칸을 채워라.\n",
    "# 조기 중단을 위한 횟수는 50회로 설정\n",
    "\n",
    "params = {'max_depth':3,\n",
    "          'eta':0.05,\n",
    "          'eval_metric':'logloss'}\n",
    "num_rounds = 400\n",
    "\n",
    "eval_list = [(dtr, 'train'), (dval, 'e                                                                                                                                                                                                                                                                                                                                                                                                                                                         al')]\n",
    "\n",
    "xgb_model = xgb.train(params = params, dtrain = dtr, num_boost_round = num_rounds,\n",
    "                      early_stopping_rounds = 50, evals = eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.002e+00 1.000e-03 9.780e-01 2.023e+00 1.000e-03 1.272e+00 2.126e+00\n",
      " 2.016e+00 1.000e-03 1.909e+00]\n"
     ]
    }
   ],
   "source": [
    "# 5. [파이썬 wrapper] predict() 함수를 적용하여 예측 확률값을 반환하고, 이를 다시 예측값으로 반환하기 위한 빈칸을 채워라.\n",
    "pred_probs = xgb_model.predict(dtest)\n",
    "print(np.round(pred_probs[:10], 3)) # predict() 수행 결과값을 10개만 표시\n",
    "\n",
    "# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 probs에 저장\n",
    "# 이 때, 리스트 내포 내의 if-else 문을 사용하여라.\n",
    "preds = [1 if x>0.5 else 0 for x in pred_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# 6. 정확도를 이용한 예측 평가를 위해 빈 칸을 채워라.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
